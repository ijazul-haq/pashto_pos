{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c5e4e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, winsound, time, joblib\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "from sklearn_crfsuite import CRF, metrics\n",
    "from nltk.tag.util import untag\n",
    "np.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90ab750",
   "metadata": {},
   "source": [
    "### Loading the manually corrected lexicon-based dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f4c015",
   "metadata": {},
   "source": [
    "the model should be trained on manually corrected lexicon-based annotated dataset. The model here is trained on only 2000 sentences that are not manually corrected. The pre-trained model is trained on around 30 sentences that are manully corrected after lexicon-based tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81ec5653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total:  2000\n",
      "Train:  1600\n",
      "Test:  400\n",
      "__________\n",
      "Total Words:  37694\n"
     ]
    }
   ],
   "source": [
    "# load the manually corrected lexicon-based annotated corpus\n",
    "df = pd.read_csv('data/lexicon_based_annotated_sample.csv', header=None)\n",
    "tagged_sentences = []\n",
    "words_count = 0\n",
    "for i, row in df.iterrows():\n",
    "    sent_id = str(row[0])\n",
    "    sentence = row[1]\n",
    "    sentence = sentence.split('|')\n",
    "    tagged_sent = []\n",
    "    for pair in sentence:\n",
    "        words_count += 1\n",
    "        pair = pair.split('_')\n",
    "        word =  pair[0]\n",
    "        tag = pair[1]\n",
    "        tag = tag.replace('.','')\n",
    "        tag = tag.replace('NNC1M', 'NNM')\n",
    "        tag = tag.replace('NNC2', 'NNS')\n",
    "        tag = tag.replace('NNC1F', 'NNF')\n",
    "        el = (word, tag)\n",
    "        tagged_sent.append(el)\n",
    "    tagged_sentences.append(tagged_sent)\n",
    "\n",
    "cutoff = int(.80 * len(tagged_sentences))\n",
    "train = tagged_sentences[:cutoff]\n",
    "test = tagged_sentences[cutoff:]\n",
    "\n",
    "print('Total: ', len(tagged_sentences))\n",
    "print('Train: ', len(train))\n",
    "print('Test: ', len(test))\n",
    "print('__________\\nTotal Words: ', words_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17004639",
   "metadata": {},
   "source": [
    "### Features Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4546ffb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def features(s, i): #s: sentence, i: word index\n",
    "    word = s[i]\n",
    "    length = len(word)\n",
    "    len_prev_1, len_prev_2, len_next_1, len_next_2 = 0,0,0,0\n",
    "    \n",
    "    prefix_1, prefix_2, prefix_3, suffix_1, suffix_2, suffix_3 = '','','','','',''\n",
    "    prev_1, prev_2, prev_3, next_1, next_2, next_3 = '','','','','',''\n",
    "    prev_1_prefix_1, prev_1_prefix_2, prev_1_suffix_1, prev_1_suffix_2, prev_1_suffix_3 = '','','','',''\n",
    "    next_1_prefix_1, next_1_prefix_2, next_1_suffix_1, next_1_suffix_2, next_1_suffix_3 = '','','','',''\n",
    "    \n",
    "    if(len(word)>1):\n",
    "        prefix_1 = word[0]\n",
    "        suffix_1 = word[-1]\n",
    "    \n",
    "    if(len(word)>2):\n",
    "        prefix_2 = word[:2]\n",
    "        suffix_2 = word[-2:]\n",
    "    \n",
    "    if(len(word)>3):\n",
    "        prefix_3 = word[:3]\n",
    "        suffix_3 = word[-3:]\n",
    "    \n",
    "    if(i>0): \n",
    "        prev_1 = s[i-1]\n",
    "        len_prev_1 = len(prev_1)\n",
    "        if(len_prev_1>1):\n",
    "            prev_1_prefix_1 = prev_1[0]\n",
    "            prev_1_suffix_1 = prev_1[-1]\n",
    "        if(len_prev_1>2):\n",
    "            prev_1_prefix_2 = prev_1[:2]\n",
    "            prev_1_suffix_2 = prev_1[-2:]\n",
    "        if(len_prev_1>3):\n",
    "            prev_1_prefix_3 = prev_1[:3]\n",
    "            prev_1_suffix_3 = prev_1[-3:]\n",
    "            \n",
    "    if(i>1): \n",
    "        prev_2 = s[i-2]\n",
    "        len_prev_2 = len(prev_2)\n",
    "    if(i>2): prev_3 = s[i-3]\n",
    "    \n",
    "    if(len(s)>i+1): \n",
    "        next_1 = s[i+1]\n",
    "        len_next_1 = len(next_1)\n",
    "        if(len_next_1>1):\n",
    "            next_1_prefix_1 = next_1[0]\n",
    "            next_1_suffix_1 = next_1[-1]\n",
    "        if(len_next_1>2):\n",
    "            next_1_prefix_2 = next_1[:2]\n",
    "            next_1_suffix_2 = next_1[-2:]  \n",
    "        if(len_next_1>3):\n",
    "            next_1_prefix_3 = next_1[:3]\n",
    "            next_1_suffix_3 = next_1[-3:]    \n",
    "        \n",
    "    if(len(s)>i+2): \n",
    "        next_2 = s[i+2]\n",
    "        len_next_2 = len(next_2)\n",
    "    if(len(s)>i+3): next_3 = s[i+3]\n",
    "    \n",
    "    features  = {\n",
    "#         word attributes\n",
    "        'word': word,\n",
    "        'length': length,\n",
    "        'is_first': i == 0,\n",
    "        'is_last': i == len(s) - 1, \n",
    "        'is_numeric': word.isdigit(),\n",
    "        'prefix_1': prefix_1,\n",
    "        'prefix_2': prefix_2,\n",
    "        'suffix_1': suffix_1,\n",
    "        'suffix_2': suffix_2,\n",
    "        \n",
    "#         previsous words attributes\n",
    "        'prev_1': prev_1,   \n",
    "        'len_prev_1': len_prev_1,     \n",
    "        'prev_1_prefix_1': prev_1_prefix_1,\n",
    "        'prev_1_prefix_2': prev_1_prefix_2,\n",
    "        'prev_1_suffix_1': prev_1_suffix_1,\n",
    "        'prev_1_suffix_2': prev_1_suffix_2, \n",
    "#         'prev_1_suffix_3': prev_1_suffix_3, \n",
    "        'prev_2': prev_2,   \n",
    "        'len_prev_2': len_prev_2,\n",
    "        'prev_3': prev_3, \n",
    "        \n",
    "#         next words attributes       \n",
    "        'next_1': next_1,  \n",
    "        'len_next_1': len_next_1, \n",
    "        'next_1_prefix_1': next_1_prefix_1,     \n",
    "        'next_1_prefix_2': next_1_prefix_2,     \n",
    "        'next_1_suffix_1': next_1_suffix_1,     \n",
    "        'next_1_suffix_2': next_1_suffix_2,  \n",
    "#         'next_1_suffix_3': next_1_suffix_3,  \n",
    "        'next_2': next_2,\n",
    "        'len_next_2': len_next_2,\n",
    "        'next_3': next_3,\n",
    "        \n",
    "      }\n",
    "    return features\n",
    "\n",
    "def transform_to_dataset(tagged_sentences):\n",
    "    X, y = [], []\n",
    "    for tagged in tagged_sentences:\n",
    "        X.append([features(untag(tagged), index) for index in range(len(tagged))])\n",
    "        y.append([tag for _, tag in tagged])\n",
    "\n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = transform_to_dataset(train)\n",
    "X_test, y_test = transform_to_dataset(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe641c2",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "827626da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:209: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  warnings.warn('From version 0.24, get_params will raise an '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CRF(algorithm='lbfgs', all_possible_transitions=True, c1=0.1, c2=0.1,\n",
       "    keep_tempfiles=None, max_iterations=100)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CRF(algorithm = 'lbfgs', c1 = 0.1, c2 = 0.1, max_iterations = 100, all_possible_transitions=True)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d0fbb178",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pashto_POS_Tagger.sav']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the model\n",
    "joblib.dump(model, 'Pashto_POS_Tagger_.sav')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae67ec71",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91138466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc =  0.9386836180638326\n",
      "f1 =  0.9382305670299027\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "\n",
    "acc = metrics.flat_accuracy_score(y_test, y_pred)\n",
    "f1 = metrics.flat_f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print('acc = ', acc)\n",
    "print('f1 = ', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92410a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n",
      "['BA', 'NB', 'RB', 'VBD', 'VBDC', 'VBDX', 'VBG', 'VBH', 'VBIMP', 'VBINF', 'VBN', 'VBP', 'VBPC', 'VBPX', 'CC', 'NG', 'UH', 'JJ', 'IN', 'NNF', 'NNM', 'NNP', 'NNS', 'RP', 'PRC', 'PRDEM', 'PRDIS', 'PRP$', 'PRPi', 'PRPii', 'PRPiii', 'PRQ', 'DT', 'PT', 'PU', 'FW', 'FX']\n",
      "Report: \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          BA      1.000     1.000     1.000        31\n",
      "          NB      0.963     0.917     0.939       169\n",
      "          RB      0.942     0.940     0.941       448\n",
      "         VBD      0.833     0.840     0.837       125\n",
      "        VBDC      1.000     1.000     1.000        36\n",
      "        VBDX      0.974     0.995     0.984       191\n",
      "         VBG      0.932     0.965     0.948        57\n",
      "         VBH      0.910     0.947     0.928        75\n",
      "       VBIMP      0.857     0.857     0.857         7\n",
      "       VBINF      0.882     0.957     0.918        94\n",
      "         VBN      0.875     0.583     0.700        12\n",
      "         VBP      0.974     0.908     0.940       163\n",
      "        VBPC      0.994     0.994     0.994       167\n",
      "        VBPX      1.000     0.909     0.952        33\n",
      "          CC      0.995     0.995     0.995       431\n",
      "          NG      1.000     0.667     0.800         3\n",
      "          UH      0.000     0.000     0.000         2\n",
      "          JJ      0.876     0.867     0.872       866\n",
      "          IN      1.000     1.000     1.000      1211\n",
      "         NNF      0.876     0.903     0.889       546\n",
      "         NNM      0.887     0.921     0.903       781\n",
      "         NNP      0.890     0.885     0.888       486\n",
      "         NNS      0.912     0.914     0.913       555\n",
      "          RP      1.000     0.967     0.983        30\n",
      "         PRC      1.000     1.000     1.000        83\n",
      "       PRDEM      0.000     0.000     0.000         0\n",
      "       PRDIS      1.000     0.667     0.800         6\n",
      "        PRP$      1.000     0.667     0.800         6\n",
      "        PRPi      1.000     0.625     0.769         8\n",
      "       PRPii      1.000     0.500     0.667         2\n",
      "      PRPiii      1.000     0.944     0.971        36\n",
      "         PRQ      1.000     0.833     0.909         6\n",
      "          DT      0.996     1.000     0.998       268\n",
      "          PT      0.998     0.994     0.996       515\n",
      "          PU      1.000     1.000     1.000        77\n",
      "          FW      0.000     0.000     0.000         2\n",
      "          FX      0.929     0.565     0.703        23\n",
      "\n",
      "   micro avg      0.939     0.939     0.939      7551\n",
      "   macro avg      0.878     0.803     0.832      7551\n",
      "weighted avg      0.939     0.939     0.938      7551\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass labels=['BA', 'NB', 'RB', 'VBD', 'VBDC', 'VBDX', 'VBG', 'VBH', 'VBIMP', 'VBINF', 'VBN', 'VBP', 'VBPC', 'VBPX', 'CC', 'NG', 'UH', 'JJ', 'IN', 'NNF', 'NNM', 'NNP', 'NNS', 'RP', 'PRC', 'PRDEM', 'PRDIS', 'PRP$', 'PRPi', 'PRPii', 'PRPiii', 'PRQ', 'DT', 'PT', 'PU', 'FW', 'FX'] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "labels = list(model.classes_)\n",
    "print(len(labels))\n",
    "sorted_labels = sorted(labels, key=lambda name: (name[1:], name[0]))\n",
    "print(sorted_labels)\n",
    "print('Report: \\n\\n{}'.format(metrics.flat_classification_report(y_test, y_pred, labels=sorted_labels, digits=3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eca2381e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-10 most likely transitions\n",
      "NB\t->\tFX\t2.15\n",
      "VBH\t->\tVBH\t1.57\n",
      "NB\t->\tNB\t1.46\n",
      "NG\t->\tFX\t1.31\n",
      "FW\t->\tUH\t1.22\n",
      "NB\t->\tNNS\t1.2\n",
      "NG\t->\tNNP\t1.17\n",
      "NNP\t->\tNNP\t1.03\n",
      "JJ\t->\tNNS\t1.03\n",
      "FX\t->\tVBIMP\t1.02\n",
      "\n",
      "Top-10 most unlikely transitions\n",
      "IN\t->\tIN\t-1.06\n",
      "NB\t->\tVBG\t-1.06\n",
      "VBG\t->\tNNS\t-1.06\n",
      "VBH\t->\tIN\t-1.07\n",
      "VBH\t->\tNNF\t-1.12\n",
      "IN\t->\tVBD\t-1.12\n",
      "VBINF\t->\tPT\t-1.18\n",
      "NB\t->\tVBD\t-1.19\n",
      "NNP\t->\tVBDX\t-1.26\n",
      "IN\t->\tCC\t-1.67\n"
     ]
    }
   ],
   "source": [
    "def print_transitions(transitions):\n",
    "    for (label_from, label_to), weight in transitions:\n",
    "        transition = label_from + '\\t->\\t' + label_to + '\\t' + str(round(weight,2))\n",
    "        print(transition)\n",
    "    \n",
    "likly = Counter(model.transition_features_).most_common(10)\n",
    "unlikly = Counter(model.transition_features_).most_common()[-10:]\n",
    "\n",
    "print('Top-10 most likely transitions')\n",
    "print_transitions(likly)\n",
    "print('\\nTop-10 most unlikely transitions')\n",
    "print_transitions(unlikly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47898e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
